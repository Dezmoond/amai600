import asyncio
import asyncpg
import os
import logging
import pandas as pd
from datetime import datetime, timedelta, timezone
from collections import deque
from typing import Union, Optional, Any, List, Dict
from features.logging import setup_logger
from preprocessing import main_preprocessing
import numpy as np
import json
import joblib

UTC = timezone.utc  # –°–æ–∑–¥–∞–µ–º –∞–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

logger = setup_logger()

DB_DSN = os.getenv('DB_DSN', 'postgresql://student:5SxdeChZ@emcable.teledesk.ru:15432/mscada_db')
CONTROL_TAG = 602
DATA_TAGS = [34, 565, 566, 567, 568, 569, 570, 571, 600, 601, 602, 603, 604]

class PostgresClient:
    def __init__(self, dsn: str):
        self.dsn = dsn
        self.pool: Optional[asyncpg.pool.Pool] = None

    async def connect(self):
        self.pool = await asyncpg.create_pool(dsn=self.dsn, min_size=1, max_size=5)
        logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö PostgreSQL —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ–≥–∞ —Å–Ω—è—Ç–∏—è —Å–æ —Å—Ç–æ–ø–∞ –ª–∏–Ω–∏–∏
    async def fetch_tag_value(self, tag_id: int) -> int:
        sql = f'''SELECT 
                    archive_itemid, 
                    source_time, 
                    value
                FROM data_raw
                WHERE
                    archive_itemid = {tag_id} AND layer = 0
                ORDER BY source_time DESC LIMIT 1'''
        
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(sql)
            if row:
                return (int(row['value']), row['source_time'])
            else:
                logger.warning(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–≥–∞ {tag_id} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return (0, 0)

    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π —Ç–µ–≥–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥ —Å–Ω—è—Ç–∏—è —Å–æ —Å—Ç–æ–ø–∞ –ø–æ—Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏
    async def fetch_multiple_tags(self, tag_ids: List[int], date_begin: str) -> Dict[int, Dict[str, Any]]:
        placeholders = ','.join(f'${i+1}' for i in range(len(tag_ids)))
        date_now = int((datetime.now().timestamp() + 11644473600) * 10000000)
        sql = f'''SELECT 
                    archive_itemid, 
                    source_time, 
                    value
                FROM data_raw
                WHERE
                    archive_itemid IN ({placeholders}) 
                    AND layer IN (0, 1)
                    AND source_time between {date_begin} and {date_now} 
                    ORDER BY source_time '''

        async with self.pool.acquire() as conn:
            rows = await conn.fetch(sql, *tag_ids)
            logger.debug(f"Fetched {len(rows)} tag values")
            #return {'archive_itemid': r['archive_itemid'], 'value': r['value'], 'timestamp': r['source_time']} for r in rows}
            data_dict = {
                'archive_itemid': [],
                'value': [],
                'timestamp': []
            }
            for r in rows:
                data_dict['archive_itemid'].append(r['archive_itemid'])
                data_dict['value'].append(r['value'])
                data_dict['timestamp'].append(r['source_time'])

            
            return data_dict


class DataCollector:
    def __init__(self, db: PostgresClient):
        self.db = db
        self.collecting = False
        self.queue: asyncio.Queue = asyncio.Queue()
        self._collector_task: Optional[asyncio.Task] = None
        self.current_session = None
        self.sessions = []

    async def start(self):
        await self.db.connect()
        logger.info("–°–±–æ—Ä—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—â–µ–Ω")
        asyncio.create_task(self._monitor_control())

    async def _monitor_control(self):
        start_ts = 0
        while True:
            val, timestamp = await self.db.fetch_tag_value(CONTROL_TAG)
            logger.info(
                f"–ó–ù–ê–ß–ï–ù–ò–ï –¢–ï–ì–ê 602: {val}, timestamp: {timestamp} ({datetime.fromtimestamp((timestamp / 10000000) - 11644473600)})")
            start_ts = timestamp
            if val == 1 and not self.collecting:
                self.collecting = True
                logger.info(f"–ù–ê–ß–ê–õ–û –°–ë–û–†–ê –î–ê–ù–ù–´–•: timestamp: {timestamp}, start_ts: {start_ts}")
                # –ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏
                self.current_session = {
                    "start_time": datetime.now(UTC).isoformat(),
                    "end_time": None,
                    "real_emergencies": 0,
                    "predicted_emergencies": 0
                }
                self._collector_task = asyncio.create_task(self._collect_loop(start_ts))
            elif val == 0 and self.collecting:
                self.collecting = False
                logger.info(f"–ó–ê–í–ï–†–®–ï–ù–ò–ï –°–ë–û–†–ê –î–ê–ù–ù–´–•: timestamp: {timestamp}, start_ts: {start_ts}")
                # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏
                if self.current_session:
                    self.current_session["end_time"] = datetime.now(UTC).isoformat()
                    self.sessions.append(self.current_session)
                    self._save_sessions()
                    self.current_session = None
                if self._collector_task:
                    self._collector_task.cancel()
            await asyncio.sleep(1)

    def _save_sessions(self):
        try:
            filename = "Session_logs.json"
            existing = []
            if os.path.exists(filename):
                with open(filename, "r") as f:
                    existing = json.load(f)

            all_sessions = existing + self.sessions
            with open(filename, "w") as f:
                json.dump(all_sessions, f, indent=2, default=str)

            self.sessions = []
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å–µ—Å—Å–∏—è –≤ {filename}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–π: {str(e)}")

    def add_real_emergency(self):
        if self.current_session:
            self.current_session["real_emergencies"] += 1

    def add_predicted_emergency(self):
        if self.current_session:
            self.current_session["predicted_emergencies"] += 1

    async def _collect_loop(self, timestamp: str):
        try:
            while self.collecting:
                data = await self.db.fetch_multiple_tags(DATA_TAGS, timestamp)
                timestamped = {
                    'collected_at': datetime.now(UTC).isoformat(),
                    'data': data
                }
                await self.queue.put(timestamped)
                # Log data collection
                logger.info(f"–°–æ–±—Ä–∞–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(timestamped['data']['value'])}")

                await asyncio.sleep(1)
        except asyncio.CancelledError:
            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–æ—Ç–æ–∫–∞ collector
            logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–æ—Ç–æ–∫–∞ collector")
            pass


class PredictionLogger:
    def __init__(self):
        self.predictions = []
        self.history = []
        self.confirmed_predictions = []
        self.false_predictions = []
        self.logger = logging.getLogger('PredictionLogger')
        self._setup_logger()

    def _setup_logger(self):
        self.logger.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(message)s')

        # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
        ch = logging.StreamHandler()
        ch.setFormatter(formatter)
        self.logger.addHandler(ch)

    def add_prediction(self, timestamp, lead_time, confidence, emergency_detected):
        prediction_record = {
            'timestamp': timestamp.isoformat(),
            'lead_time_sec': lead_time,
            'confidence': round(confidence, 4),
            'is_emergency_predicted': emergency_detected,
            'is_emergency_actual': None,
            'predicted_time': (timestamp + pd.Timedelta(seconds=lead_time)).strftime('%H:%M:%S'),
            'is_confirmed': None  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±–∞ —Å–ø–∏—Å–∫–∞ (history –∏ predictions)
        self.history.append(prediction_record)
        self.predictions.append(prediction_record)  # –ï—Å–ª–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        self.logger.info(
            f"–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï: {'üö® –ê–≤–∞—Ä–∏—è' if emergency_detected else '–ù–µ—Ç –∞–≤–∞—Ä–∏–∏'} "
            f"—á–µ—Ä–µ–∑ {lead_time} —Å–µ–∫ "
            f"(–≤ {prediction_record['predicted_time']}, "
            f"—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {confidence:.1%})"
        )

    def check_predictions(self, event_time):
        for pred in self.predictions:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–ª–µ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if 'is_confirmed' not in pred:
                pred['is_confirmed'] = None

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ datetime
            try:
                pred_timestamp = datetime.fromisoformat(pred['timestamp'])
                predicted_time = pred_timestamp + timedelta(seconds=pred['lead_time_sec'])
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–∏: {str(e)}")
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É —Å —Å–æ–±—ã—Ç–∏–µ–º
            time_diff = (event_time - predicted_time).total_seconds()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π
            if abs(time_diff) <= 2:
                pred['is_confirmed'] = True
                self.confirmed_predictions.append(pred)
                self.logger.warning(
                    f"‚úÖ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞ {pred['lead_time_sec']} —Å–µ–∫ "
                    f"(–æ–±—ä—è–≤–ª–µ–Ω–æ {pred_timestamp.strftime('%H:%M:%S')})"
                )
            elif event_time > predicted_time:
                pred['is_confirmed'] = False
                self.false_predictions.append(pred)
                self.logger.error(
                    f"‚ùå –õ–û–ñ–ù–û–ï: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ {pred_timestamp.strftime('%H:%M:%S')} "
                    f"–æ–∂–∏–¥–∞–ª–æ —Å–æ–±—ã—Ç–∏–µ –∫ {predicted_time.strftime('%H:%M:%S')}"
                )

    def log_best_prediction(self):
        if self.confirmed_predictions:
            best = max(self.confirmed_predictions, key=lambda x: x['lead_time'])
            self.logger.warning(
                f"üèÜ –õ–£–ß–®–ï–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï: –ó–∞ {best['lead_time']} —Å–µ–∫ "
                f"(—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {best['confidence']:.1%})"
            )

    def print_current_status(self):
        current_time = datetime.now()

        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª—é—á–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏
        active = [
            p for p in self.predictions
            if p.get('is_confirmed') is None
               and datetime.strptime(p['predicted_time'], '%H:%M:%S').replace(year=current_time.year,
                                                                              month=current_time.month,
                                                                              day=current_time.day) > current_time
        ]

        self.logger.info("\n=== –¢–ï–ö–£–©–ò–ô –°–¢–ê–¢–£–° –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô ===")

        if active:
            self.logger.info("–ê–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:")
            for pred in sorted(active, key=lambda x: x['lead_time_sec'], reverse=True):
                # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä–µ–∫—Ç datetime –∏–∑ —Å—Ç—Ä–æ–∫–∏
                predicted_time = datetime.strptime(pred['predicted_time'], '%H:%M:%S').replace(
                    year=current_time.year,
                    month=current_time.month,
                    day=current_time.day
                )
                sec_left = (predicted_time - current_time).total_seconds()
                self.logger.info(
                    f"‚Ä¢ –ß–µ—Ä–µ–∑ {sec_left:.1f} —Å–µ–∫ (–∑–∞–ø–∞—Å –≤—Ä–µ–º–µ–Ω–∏ {pred['lead_time_sec']} —Å–µ–∫, "
                    f"—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {pred['confidence']:.1%})"
                )
        else:
            self.logger.info("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")

        self.logger.info("===============================\n")

    def save_history(self, filename, mode='a'):
        try:
            # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ —Ñ–∞–π–ª –µ—Å—Ç—å
            existing_data = []
            if os.path.exists(filename):
                with open(filename, 'r', encoding='utf-8') as f:
                    try:
                        existing_data = json.load(f)
                    except json.JSONDecodeError:
                        existing_data = []

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if isinstance(existing_data, list):
                existing_data.extend(self.predictions)
            else:
                existing_data = self.predictions

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, default=str, ensure_ascii=False)

            self.logger.info(f"–ò—Å—Ç–æ—Ä–∏—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∞ –≤ {filename}")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")

class DataBuffer:
    def __init__(self, max_length=150):
        self.max_length = max_length
        self.buffer = deque(maxlen=max_length)
        self.last_valid_values = {}  # –•—Ä–∞–Ω–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —Å—Ç–æ–ª–±—Ü–æ–≤

    def update(self, new_data: pd.DataFrame):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –±—É—Ñ–µ—Ä, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—Å–µ—Ö —Å—Ç–æ–ª–±—Ü–æ–≤"""
        # 1. –û–±–Ω–æ–≤–ª—è–µ–º last_valid_values –¥–ª—è –≤—Å–µ—Ö —Å—Ç–æ–ª–±—Ü–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ new_data
        for col in new_data.columns:
            # –ï—Å–ª–∏ —Å—Ç–æ–ª–±–µ—Ü –ø–æ–ª–Ω–æ—Å—Ç—å—é NaN - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            if new_data[col].isna().all():
                if col in self.last_valid_values:
                    new_data[col] = self.last_valid_values[col]
                else:
                    new_data[col] = 0  # –ò–ª–∏ –¥—Ä—É–≥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤–∞–ª–∏–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                last_val = new_data[col].last_valid_index()
                if last_val is not None:
                    self.last_valid_values[col] = new_data[col].iloc[last_val]

        # 2. –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –±—É—Ñ–µ—Ä
        self.buffer.extend(new_data.to_dict('records'))

        # 3. –î–æ–∑–∞–ø–æ–ª–Ω—è–µ–º –±—É—Ñ–µ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if len(self.buffer) < self.max_length and self.last_valid_values:
            missing = self.max_length - len(self.buffer)
            filler = {k: [v] * missing for k, v in self.last_valid_values.items()}
            self.buffer.extendleft(pd.DataFrame(filler).to_dict('records')[::-1])

        return pd.DataFrame(self.buffer)


model = joblib.load('random_forestFULLDATASET11sd102min.pkl')
scaler = joblib.load('scalerFULLDATASET11sd102min.pkl')


async def model_worker(queue: asyncio.Queue, collector: DataCollector):
    logger.info("–ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    buffer = deque(maxlen=150)
    prediction_logger = PredictionLogger()

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤
    NUMERIC_TAGS = [34, 565, 566, 567, 568, 569, 570, 571, 603, 604]
    CLASS_TAGS = {600: 0, 601: 0, 602: 1}
    ALL_TAGS = NUMERIC_TAGS + list(CLASS_TAGS.keys())
    ALL_PREDICTIONS_FILE = 'all_predictions.json'
    ##########iteration_counter = 0
    try:
        while True:
            ##########iteration_counter += 1
            item = await queue.get()
            current_time = None

            try:
                # === –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥—è—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö ===
                raw_data = item['data']

                # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
                chunk_df = pd.DataFrame({
                    'timestamp': raw_data['timestamp'],
                    'archive_itemid': raw_data['archive_itemid'],
                    'value': raw_data['value']
                })

                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ Windows FILETIME
                chunk_df['timestamp'] = chunk_df['timestamp'].apply(
                    lambda x: datetime.fromtimestamp((x / 10 ** 7) - 11644473600, tz=UTC)
                )
                chunk_df['timestamp'] = pd.to_datetime(chunk_df['timestamp'])

                # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ ===
                # –°–æ–∑–¥–∞–Ω–∏–µ pivot —Ç–∞–±–ª–∏—Ü—ã
                pivot_df = chunk_df.pivot_table(
                    index='timestamp',
                    columns='archive_itemid',
                    values='value',
                    aggfunc='last'
                ).resample('1s').last()

                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ç–µ–≥–æ–≤
                for tag in ALL_TAGS:
                    if tag not in pivot_df.columns:
                        pivot_df[tag] = np.nan

                # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
                pivot_df = pivot_df.rename(columns=lambda x: f"value_{x}")
                processed_df = pivot_df.reset_index().rename(columns={'timestamp': 'source_time'})

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–ª—å—Ü–µ–≤–æ–≥–æ –±—É—Ñ–µ—Ä–∞
                buffer.extend(processed_df.to_dict('records'))
                processed_df = pd.DataFrame(buffer)

                # === –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ===
                # –ß–∏—Å–ª–æ–≤—ã–µ —Ç–µ–≥–∏
                for tag in NUMERIC_TAGS:
                    col = f"value_{tag}"
                    if col in processed_df.columns:
                        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∫ —Ä–∞–±–æ—á–µ–π –≤–µ—Ä—Å–∏–∏
                        processed_df[col] = processed_df[col].interpolate(
                            method='linear',  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å 'time' –Ω–∞ 'linear'
                            limit_direction='both'
                        )

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –æ—Ç—Å—Ç—É–ø—ã)
                for tag, default_val in CLASS_TAGS.items():
                    col = f"value_{tag}"
                    if col in processed_df.columns:
                        processed_df[col] = processed_df[col].ffill().fillna(default_val)

                # === –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ===
                current_time = processed_df['source_time'].iloc[-1]

                try:
                    # === –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ===
                    current_time = processed_df['source_time'].iloc[-1]

                    # –í—Å–µ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    prediction_df = prepare_prediction_steps(processed_df, steps=10)

                    if prediction_df.isnull().any().any():
                        logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö - –∑–∞–º–µ–Ω–∞ —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏")
                        prediction_df = prediction_df.fillna(prediction_df.mean())

                    scaled = scaler.transform(prediction_df)
                    predictions = model.predict(scaled)
                    prediction_probas = model.predict_proba(scaled)

                    # –ò–º–∏—Ç–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–π 5-–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                    ##########if iteration_counter % 5 == 0:
                    ##########    logger.warning("!!! –¢–ï–°–¢–û–í–û–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –ê–í–ê–†–ò–ò !!!")
                    ##########    predictions = np.array([1] * 10)  # –í—Å–µ 10 —à–∞–≥–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç –∞–≤–∞—Ä–∏—é
                    ##########    prediction_probas = np.array([[0.0, 1.0]] * 10)  # 100% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∞–≤–∞—Ä–∏–∏
                    # === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ª–∏—á–∏—è –∞–≤–∞—Ä–∏–π–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ===
                    has_emergency_prediction = any(predictions)

                    # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∞–≤–∞—Ä–∏–∏ ===
                    if has_emergency_prediction:
                        emergency_detected = False
                        for i, (pred, proba) in enumerate(zip(predictions, prediction_probas)):
                            if pred == 1:
                                lead_time = 10 - i
                                confidence = proba[1]

                                prediction_logger.add_prediction(
                                    timestamp=current_time,
                                    lead_time=lead_time,
                                    confidence=confidence,
                                    emergency_detected=True
                                )
                                emergency_detected = True

                        if emergency_detected:
                            prediction_logger.save_history(ALL_PREDICTIONS_FILE)

                    # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –†–ï–ê–õ–¨–ù–´–ï –∞–≤–∞—Ä–∏–π–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è ===
                    is_emergency = (
                            processed_df['value_600'].iloc[-1] == 1 or
                            processed_df['value_601'].iloc[-1] == 1
                    )

                    if is_emergency:
                        collector.add_real_emergency()  # <-- –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—É—é –∞–≤–∞—Ä–∏—é
                        logger.warning(f"!!! –ê–í–ê–†–ò–Ø –û–ë–ù–ê–†–£–ñ–ï–ù–ê –í {current_time.strftime('%H:%M:%S')} !!!")
                        prediction_logger.check_predictions(current_time)
                        prediction_logger.log_best_prediction()
                        prediction_logger.save_history('emergency_predictions.json')

                    # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∞–≤–∞—Ä–∏–∏ ===
                    if has_emergency_prediction:
                        collector.add_predicted_emergency()  # <-- –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é –∞–≤–∞—Ä–∏—é
                        emergency_detected = False

                    # === –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞ (–¥–∞–∂–µ –±–µ–∑ –∞–≤–∞—Ä–∏–π) ===
                    if len(buffer) % 50 == 0:
                        prediction_logger.print_current_status()
                        prediction_logger.save_history('predictions_history.json')

                    # === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
                    # –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –í–°–ï–ì–î–ê
                    print("\n" + "=" * 80)
                    print(f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ({current_time.strftime('%H:%M:%S')}):")

                    display_data = {
                        '–®–∞–≥': range(10, 0, -1),
                        '–ü—Ä–æ–≥–Ω–æ–∑ –∞–≤–∞—Ä–∏–∏': ['–î–∞' if p == 1 else '–ù–µ—Ç' for p in predictions],
                        '–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–Ω–æ—Ä–º–∞)': [f"{p[0]:.2%}" for p in prediction_probas],
                        '–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–∞–≤–∞—Ä–∏—è)': [f"{p[1]:.2%}" for p in prediction_probas]
                    }

                    display_df = pd.DataFrame(display_data)
                    print(display_df.to_string(index=False, justify='center'))
                    print("=" * 80 + "\n")

                    # –í—ã–≤–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö –í–°–ï–ì–î–ê
                    output_df = processed_df.tail(10).copy()
                    output_df['source_time'] = output_df['source_time'].dt.strftime('%Y-%m-%d %H:%M:%S')

                    print("\n" + "=" * 100)
                    print(f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ–∫–∞–∑–∞–Ω–∏—è (10 –∏–∑ {len(processed_df)} –∑–∞–ø–∏—Å–µ–π):")
                    print(output_df.fillna('N/A').to_string(index=False))
                    print("=" * 100 + "\n")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON –¢–û–õ–¨–ö–û –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∞–≤–∞—Ä–∏–π–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                    if has_emergency_prediction:
                        prediction_logger.save_history(ALL_PREDICTIONS_FILE)
                except Exception as model_error:
                    logger.error(f"–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏: {str(model_error)}", exc_info=True)

            except Exception as processing_error:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {str(processing_error)}", exc_info=True)
            finally:
                queue.task_done()

    except asyncio.CancelledError:
        logger.info("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        prediction_logger.save_history('shutdown_predictions.json')
        raise

    except Exception as fatal_error:
        logger.critical(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {str(fatal_error)}", exc_info=True)
        prediction_logger.save_history('crash_dump.json')
        raise

def prepare_prediction_steps(processed_df: pd.DataFrame, steps: int = 10) -> pd.DataFrame:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç N –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –≤–ø–µ—Ä—ë–¥.
    –ö–∞–∂–¥—ã–π —à–∞–≥ ‚Äî –æ—Ç–¥–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ 1, 2, ..., N —Å–µ–∫—É–Ω–¥ –¥–æ —Å–æ–±—ã—Ç–∏—è.
    """
    df = processed_df.copy().rename(columns=lambda x: x.replace('value_', ''))
    df = df.set_index('source_time').sort_index()

    window_params = {
        '5s': {'min_periods': 5, 'fill_method': 'linear'},
        '50s': {'min_periods': 50, 'fill_method': 'ffill'},
        '2min': {'min_periods': 120, 'fill_method': 'bfill'}
    }

    for window, params in window_params.items():
        ma_col = f'34_ma_{window}'
        std_col = f'34_std_{window}'
        df[ma_col] = df['34'].rolling(window, min_periods=params['min_periods']).mean()
        df[std_col] = df['34'].rolling(window, min_periods=params['min_periods']).std()

        if params['fill_method'] == 'ffill':
            df[[ma_col, std_col]] = df[[ma_col, std_col]].ffill()
        elif params['fill_method'] == 'bfill':
            df[[ma_col, std_col]] = df[[ma_col, std_col]].bfill()
        else:
            df[[ma_col, std_col]] = df[[ma_col, std_col]].interpolate()

    df = df.reset_index()

    feature_columns = ['604', '603', '602', '571', '570', '569', '568',
                       '567', '566', '565', '34', '34_ma_5s', '34_std_5s',
                       '34_ma_50s', '34_std_50s', '34_ma_2min', '34_std_2min']

    for col in feature_columns:
        if col not in df.columns:
            df[col] = np.nan

    return df[feature_columns].tail(steps)





async def main():
    logger.info("–ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
    db_client = PostgresClient(DB_DSN)
    collector = DataCollector(db_client)
    await collector.start()

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ 2—Ö –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    logger.info("–ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
    workers = [asyncio.create_task(model_worker(collector.queue, collector))]
    #workers = [asyncio.create_task(model_worker(collector.queue)) for _ in range(2)]
    # –∑–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–æ–≤
    await asyncio.gather(*workers)
logger = setup_logger()

if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info('–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...')
